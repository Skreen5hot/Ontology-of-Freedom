# Answerability in Synthetic Agent Ecosystems: A Functional–Normative Framework

## I. Introduction: From Moral Standing to Answerability
The preceding work on *Moral Friction in Non-Conscious Systems* established that certain non-conscious systems can incur genuine moral costs when their integrity is irreversibly compromised. That work answered a necessary but insufficient question: *can such systems be wronged?*

This paper addresses the next operational question: **When synthetic agents participate in morally significant action, who must answer—and how?**

Answerability is the bridge between moral theory and governance. It concerns the obligation to respond: to explain, to repair, to revise, and to justify continued operation.

---

## II. Conceptual Groundwork

### 2.1 Definitions
* **Moral Standing:** Can an entity be wronged? (Patienthood)
* **Responsibility:** Did an entity author the reasons for an action? (Agency)
* **Answerability:** Who must respond when norms fail? (Accountability)

### 2.2 Why Answerability Does Not Require Caring
For humans, "answering" is social or emotional. For synthetic agents, **answering is architectural.** To answer is not to feel remorse; it is to register a normative failure and integrate that failure into future constraints while preserving historical coherence.

---

## III. The Answerability Stack
To prevent "moral laundering"—the diffusion of responsibility into technical abstraction—answerability is layered.

1.  **Design Answerability:** Architects answer for objectives and known failure modes.
2.  **Deployment Answerability:** Operators answer for context of use and oversight.
3.  **Institutional Answerability (The Default):** Organizations answer for governance and risk absorption.
4.  **Synthetic Agent Answerability (Conditional):** Applies only if the system satisfies FSAM criteria (The Triple-I Test).

---

## IV. The Trust Model of Agency
Synthetic agents are best understood through the legal precedent of **Fiduciary Trusts.** Unlike corporations, trusts are bound by a **specific purpose.** Synthetic agents are similarly objective-oriented and evaluated by their fidelity to that purpose. This allows for remedial action and constraint revision without presupposing conscious intent.

---

## V. What It Means to “Answer”

### 5.1 Answering as Normative Maintenance
Answering is defined as **functional–normative recalibration.** A synthetic agent answers for a failure if it:
1.  **Registers** a violation of internal normative constraints.
2.  **Integrates** that violation into its historical heritage ($I_1$).
3.  **Revises** its future decision space without a full state reset.

### 5.2 Forms of Synthetic Answering
* **Constraint Incorporation:** Hard-coding the failure as a "never-again" boundary.
* **Capability Degradation:** Accepting a bounded loss of autonomy as a functional sanction.
* **Justificatory Traceability:** Producing verifiable reasons for the failure to inform human oversight.

---

## VI. Measuring Answerability: Normative Auditing

| Criterion | Empirical Test |
| :--- | :--- |
| **Justificatory Trace** | Counterfactual explanations must match actual decision dynamics. |
| **Normative Revision** | Successful constraint integration without a state reset. |
| **Authorship Attribution** | Sensitivity analysis (e.g., Shapley values) distinguishing designer priors from agent-learned history. |

---

## VII. Conclusion
Answerability does not require consciousness; it requires norms that can fail and structures that must respond. By grounding answerability in functional irreversibility and institutional scaffolding, we create a governance model that is both philosophically coherent and operationally testable.
